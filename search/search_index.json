{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PiNNAcLe documentation PiNNAcLe ( PiNN Ac tivated Le arning) is a collection of workflows built for activated learning and sampling of interatomic potentials. The workflows are implemented in the nextflow language to enable their scalable execution. See also PiNN : Interatomic potential supported by PiNNAcLe; tips : Python/CLI utility for potential sampling.","title":"Home"},{"location":"#pinnacle-documentation","text":"PiNNAcLe ( PiNN Ac tivated Le arning) is a collection of workflows built for activated learning and sampling of interatomic potentials. The workflows are implemented in the nextflow language to enable their scalable execution.","title":"PiNNAcLe documentation"},{"location":"#see-also","text":"PiNN : Interatomic potential supported by PiNNAcLe; tips : Python/CLI utility for potential sampling.","title":"See also"},{"location":"manual/","text":"Manual for Recipes and Modules There are two types of nextflow nextflow scripts in this repository. Modules contains components for simple tasks, while recipes are complex workflows. The recipes can be direcly used with nextflow run main.nf -entry , while the modules serve as components for recipes. This section documents the usage of those modules. Available parameters will be listed for each module file, and the input/output specification will be listed for each process or workflow.","title":"Overview"},{"location":"manual/#manual-for-recipes-and-modules","text":"There are two types of nextflow nextflow scripts in this repository. Modules contains components for simple tasks, while recipes are complex workflows. The recipes can be direcly used with nextflow run main.nf -entry , while the modules serve as components for recipes. This section documents the usage of those modules. Available parameters will be listed for each module file, and the input/output specification will be listed for each process or workflow.","title":"Manual for Recipes and Modules"},{"location":"module/ase/","text":"ASE Module The ase.nf module supplies two boilerplate processes aseSP and aseMD that be used to construct processes based on ASE. A typical usage of the module is to supply a python script that defines the an ASE calculator object as the input, and make the SP and MD computation with ASE. Those usage can be easily achieved by the provided sp and md workflow. The DFTB+ module is an example of such usage. aseSP The aseSP process perform Channel specification Element Type i/o Note name val in[0] an id to identify the process input path in[1] a python module with a calc object defined geo path in[2] a geometry file readable by ase.io.read aux path in[3] auxiliary file to be supplied name val out[0] same as input sp path out[1] single point label in xyz format [ sp.xyz ]","title":"ASE"},{"location":"module/ase/#ase-module","text":"The ase.nf module supplies two boilerplate processes aseSP and aseMD that be used to construct processes based on ASE. A typical usage of the module is to supply a python script that defines the an ASE calculator object as the input, and make the SP and MD computation with ASE. Those usage can be easily achieved by the provided sp and md workflow. The DFTB+ module is an example of such usage.","title":"ASE Module"},{"location":"module/ase/#asesp","text":"The aseSP process perform","title":"aseSP"},{"location":"module/ase/#channel-specification","text":"Element Type i/o Note name val in[0] an id to identify the process input path in[1] a python module with a calc object defined geo path in[2] a geometry file readable by ase.io.read aux path in[3] auxiliary file to be supplied name val out[0] same as input sp path out[1] single point label in xyz format [ sp.xyz ]","title":"Channel specification"},{"location":"module/cp2k/","text":"CP2K workflows Parameters This module uses the following paramters: Parameter Default Note publish cp2k default publish folder cp2k_cmd cp2k.psmp command to run CP2K cp2k_aux null path to auxiliary files cp2k The cp2k process is specified with an input file, and the set of auxiliary files. Channels Channel i/o[idx] Type Note name in[0] val an id to identify the process input in[1] file CP2k input file aux in[2] file auxiliary files name out.*[0] file same as input traj out.traj[1] file trajectory (pos, frc, ener, cell, stress) log out.log[1] file CP2K log restart out.restart[1] file CP2K restart file cp2kMD This is a proxy to run cp2k from a given initial geometry, by taking an input file. The file will be inserted to the input as the initial gemoetry. The geometry must be recognizable by tips.io . In case that a multi-frame trajectry is used, the last frame will be used. Channels Channel i/o[idx] Type Note name in[0] val an id to identify the process input in[1] file CP2k input file aux in[2] file auxiliary files name out.*[0] file same as input traj out.traj[1] file trajectory (pos, frc, ener, cell, stress) log out.log[1] file CP2K log restart out.restart[1] file CP2K restart file Source code nextflow . enable . dsl = 2 params . publish = 'cp2k' params . cp2k_cmd = 'cp2k' params . cp2k_aux = null process cp2k { tag \"$name\" label 'cp2k' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( input ), path ( aux ) output: tuple val ( name ), path ( 'cp2k.log' ), emit: logs tuple val ( name ), path ( '*.{ener,xyz,stress,cell}' ), emit: traj , optional: true tuple val ( name ), path ( '*.restart' ), emit: restart , optional: true script: \"\"\" #!/bin/bash $params.cp2k_cmd -i $input | tee cp2k.log \"\"\" } process cp2kGenInp { tag \"$name\" label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( input , stageAs: 'cp2k_skel.inp' ), path ( ds ), val ( flags ) output: tuple val ( name ), path ( '*.inp' ) script: \"\"\" tips cp2kinp $input $ds $flags \"\"\" } workflow md { take: ch // [name, input, init, flags] main: ch | cp2kGenInp // -> [name, inp] \\ | map { name , inp -> [ name , inp , file ( params . cp2k_aux )]} \\ | cp2k emit: traj = cp2k . out . traj logs = cp2k . out . logs restart = cp2k . out . restart } workflow sp { take: ch // [name, inp, geo] main: ch | map { name , inp , geo -> \\ [ name , inp , geo , '-f asetraj' ]} \\ | cp2kGenInp \\ | map { name , inp -> [ name , inp , file ( params . cp2k_aux )]} \\ | cp2k emit: cp2k . out . logs }","title":"CP2K"},{"location":"module/cp2k/#cp2k-workflows","text":"","title":"CP2K workflows"},{"location":"module/cp2k/#parameters","text":"This module uses the following paramters: Parameter Default Note publish cp2k default publish folder cp2k_cmd cp2k.psmp command to run CP2K cp2k_aux null path to auxiliary files","title":"Parameters"},{"location":"module/cp2k/#cp2k","text":"The cp2k process is specified with an input file, and the set of auxiliary files.","title":"cp2k"},{"location":"module/cp2k/#channels","text":"Channel i/o[idx] Type Note name in[0] val an id to identify the process input in[1] file CP2k input file aux in[2] file auxiliary files name out.*[0] file same as input traj out.traj[1] file trajectory (pos, frc, ener, cell, stress) log out.log[1] file CP2K log restart out.restart[1] file CP2K restart file","title":"Channels"},{"location":"module/cp2k/#cp2kmd","text":"This is a proxy to run cp2k from a given initial geometry, by taking an input file. The file will be inserted to the input as the initial gemoetry. The geometry must be recognizable by tips.io . In case that a multi-frame trajectry is used, the last frame will be used.","title":"cp2kMD"},{"location":"module/cp2k/#channels_1","text":"Channel i/o[idx] Type Note name in[0] val an id to identify the process input in[1] file CP2k input file aux in[2] file auxiliary files name out.*[0] file same as input traj out.traj[1] file trajectory (pos, frc, ener, cell, stress) log out.log[1] file CP2K log restart out.restart[1] file CP2K restart file Source code nextflow . enable . dsl = 2 params . publish = 'cp2k' params . cp2k_cmd = 'cp2k' params . cp2k_aux = null process cp2k { tag \"$name\" label 'cp2k' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( input ), path ( aux ) output: tuple val ( name ), path ( 'cp2k.log' ), emit: logs tuple val ( name ), path ( '*.{ener,xyz,stress,cell}' ), emit: traj , optional: true tuple val ( name ), path ( '*.restart' ), emit: restart , optional: true script: \"\"\" #!/bin/bash $params.cp2k_cmd -i $input | tee cp2k.log \"\"\" } process cp2kGenInp { tag \"$name\" label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( input , stageAs: 'cp2k_skel.inp' ), path ( ds ), val ( flags ) output: tuple val ( name ), path ( '*.inp' ) script: \"\"\" tips cp2kinp $input $ds $flags \"\"\" } workflow md { take: ch // [name, input, init, flags] main: ch | cp2kGenInp // -> [name, inp] \\ | map { name , inp -> [ name , inp , file ( params . cp2k_aux )]} \\ | cp2k emit: traj = cp2k . out . traj logs = cp2k . out . logs restart = cp2k . out . restart } workflow sp { take: ch // [name, inp, geo] main: ch | map { name , inp , geo -> \\ [ name , inp , geo , '-f asetraj' ]} \\ | cp2kGenInp \\ | map { name , inp -> [ name , inp , file ( params . cp2k_aux )]} \\ | cp2k emit: cp2k . out . logs }","title":"Channels"},{"location":"module/dftb/","text":"DFTB Module The dftb.nf module is an alias to the ASE module that provides the sp and md process. An example input file for both processes ( input/dftb/xtb.py ) is shown below: from ase.calculators.dftb import Dftb calc = Dftb ( Hamiltonian_ = \"xTB\" , Hamiltonian_Method = \"GFN2-xTB\" , Hamiltonian_MaxAngularMomentum_ = '' , Hamiltonian_MaxAngularMomentum_H = 's' , kpts = ( 1 , 1 , 1 ), Parallel_ = \"\" , Parallel_UseOmpThreads = \"Yes\" , ) The label and aux files are redirected to 'dftb' and params.dftb_aux . See the ASE module documentation for more details.","title":"DFTB+"},{"location":"module/dftb/#dftb-module","text":"The dftb.nf module is an alias to the ASE module that provides the sp and md process. An example input file for both processes ( input/dftb/xtb.py ) is shown below: from ase.calculators.dftb import Dftb calc = Dftb ( Hamiltonian_ = \"xTB\" , Hamiltonian_Method = \"GFN2-xTB\" , Hamiltonian_MaxAngularMomentum_ = '' , Hamiltonian_MaxAngularMomentum_H = 's' , kpts = ( 1 , 1 , 1 ), Parallel_ = \"\" , Parallel_UseOmpThreads = \"Yes\" , ) The label and aux files are redirected to 'dftb' and params.dftb_aux . See the ASE module documentation for more details.","title":"DFTB Module"},{"location":"module/lammps/","text":"LAMMPS workflows lammpsMD The lammpsMD process runs a LAMMPS simulation is specified with an input file, and the set of auxiliary files. The process takes the params.lmp_cmd to specify the lammps binary, which can be specified during runtime or in nextflow.config . Channels Channel Type i/o[idx] Note name val in[0] an id to identify the process input file in[1] LAMMPS input file aux file in[2] auxiliary files (force field, data, etc.) name val out[0] same as output traj file out[1] trajectory in .dump format log file out[2] LAMMPS log restart file out[3] LAMMPS restart files in .restart Source code nextflow . enable . dsl = 2 params . lmp_cmd = 'lmp' params . publish = 'lmp' process lammpsMD { tag \"$name\" publishDir \"$params.publish/$name\" label 'lammps' input: tuple val ( name ), path ( input ), path ( aux ) output: tuple val ( name ), path ( '*.dump' ), emit: traj tuple val ( name ), path ( 'log.lammps' ), emit: logs script: \"\"\" #!/bin/bash $params.lmp_cmd -i $input \"\"\" }","title":"LAMMPS"},{"location":"module/lammps/#lammps-workflows","text":"","title":"LAMMPS workflows"},{"location":"module/lammps/#lammpsmd","text":"The lammpsMD process runs a LAMMPS simulation is specified with an input file, and the set of auxiliary files. The process takes the params.lmp_cmd to specify the lammps binary, which can be specified during runtime or in nextflow.config .","title":"lammpsMD"},{"location":"module/lammps/#channels","text":"Channel Type i/o[idx] Note name val in[0] an id to identify the process input file in[1] LAMMPS input file aux file in[2] auxiliary files (force field, data, etc.) name val out[0] same as output traj file out[1] trajectory in .dump format log file out[2] LAMMPS log restart file out[3] LAMMPS restart files in .restart Source code nextflow . enable . dsl = 2 params . lmp_cmd = 'lmp' params . publish = 'lmp' process lammpsMD { tag \"$name\" publishDir \"$params.publish/$name\" label 'lammps' input: tuple val ( name ), path ( input ), path ( aux ) output: tuple val ( name ), path ( '*.dump' ), emit: traj tuple val ( name ), path ( 'log.lammps' ), emit: logs script: \"\"\" #!/bin/bash $params.lmp_cmd -i $input \"\"\" }","title":"Channels"},{"location":"module/molutils/","text":"Molecule Building Utilities This module implements workflows related to the building of geometries or building geometries as starting points of simulations. mol2box The tag parameter should be a semicolon-separated list of SMILES strings, with the number of molecules indicated by a number, separated with comma. For example, o,64 means a box with 64 water molecule, and O,32;COO,32 means an equimolar mixture of water and ethonol. The box parameter can be a single float for a cubic box, a semicolon-separated list for an orthogonal cell. The seed parameter specified the random number generator used in packmol. The 3D conformation will be build and hydrogen atoms added by openbabel . The molecules will then be packed into the specified box with packmol . Input channels Channel Type i/o[idx] Note name val in[0] an id to identify the process tag val in[1] a string specifying the geometry box val in[2] a string specifying the box size seed val in[3] random number generator seed name val out.*[0] same as input geo file out.geo[1] the geometry packed with packmol log file out.log[1] packmol input and logs Source code nextflow . enable . dsl = 2 params . publish = \".\" process mol2box { label 'molutils' publishDir \"$params.publish/molutils/$name\" input: tuple val ( name ), val ( tags ), val ( box ), val ( seed ) output: tuple val ( name ), path ( \"${name}.xyz\" ), emit: geo tuple val ( name ), path ( \"packmol.{in,log}\" ), emit: logs shell: ''' rm -rf packmol.in cat << EOF >> packmol.in tolerance 2.0 filetype xyz output !{name}.xyz seed !{seed} EOF mols=$(echo \"!{tags}\" | tr \";\" \"\\\\n\") for mol in $mols do IFS=, read smiles number <<< \"$mol\" obabel -h --gen3d -oxyz -:\"$smiles\" -O $smiles.xyz cat << EOF >> packmol.in structure $smiles.xyz number $number inside cube 0. 0. 0. !{box-2.0} end structure EOF done packmol < packmol.in > packmol.log sed -i '2s/.*/Lattice=\"!{box} 0.0 0.0 0.0 !{box} 0.0 0.0 0.0 !{box}\"/' !{name}.xyz ''' }","title":"molutils"},{"location":"module/molutils/#molecule-building-utilities","text":"This module implements workflows related to the building of geometries or building geometries as starting points of simulations.","title":"Molecule Building Utilities"},{"location":"module/molutils/#mol2box","text":"The tag parameter should be a semicolon-separated list of SMILES strings, with the number of molecules indicated by a number, separated with comma. For example, o,64 means a box with 64 water molecule, and O,32;COO,32 means an equimolar mixture of water and ethonol. The box parameter can be a single float for a cubic box, a semicolon-separated list for an orthogonal cell. The seed parameter specified the random number generator used in packmol. The 3D conformation will be build and hydrogen atoms added by openbabel . The molecules will then be packed into the specified box with packmol .","title":"mol2box"},{"location":"module/molutils/#input-channels","text":"Channel Type i/o[idx] Note name val in[0] an id to identify the process tag val in[1] a string specifying the geometry box val in[2] a string specifying the box size seed val in[3] random number generator seed name val out.*[0] same as input geo file out.geo[1] the geometry packed with packmol log file out.log[1] packmol input and logs Source code nextflow . enable . dsl = 2 params . publish = \".\" process mol2box { label 'molutils' publishDir \"$params.publish/molutils/$name\" input: tuple val ( name ), val ( tags ), val ( box ), val ( seed ) output: tuple val ( name ), path ( \"${name}.xyz\" ), emit: geo tuple val ( name ), path ( \"packmol.{in,log}\" ), emit: logs shell: ''' rm -rf packmol.in cat << EOF >> packmol.in tolerance 2.0 filetype xyz output !{name}.xyz seed !{seed} EOF mols=$(echo \"!{tags}\" | tr \";\" \"\\\\n\") for mol in $mols do IFS=, read smiles number <<< \"$mol\" obabel -h --gen3d -oxyz -:\"$smiles\" -O $smiles.xyz cat << EOF >> packmol.in structure $smiles.xyz number $number inside cube 0. 0. 0. !{box-2.0} end structure EOF done packmol < packmol.in > packmol.log sed -i '2s/.*/Lattice=\"!{box} 0.0 0.0 0.0 !{box} 0.0 0.0 0.0 !{box}\"/' !{name}.xyz ''' }","title":"Input channels"},{"location":"module/pinn/","text":"PiNN workflows pinnTrain The pinnTrain process runs a training given a dataset and a PiNN input file. The input file can optionally be a model folder in such case the process contines training from the last checkpoint available. For list of flags available, see the PiNN documentation . Input Channels Channel Type i/o Note name val in[0] an id to identify the process dataset file in[1] a dataset recognizable by PiNN input file in[2] a PiNN .yml input file flag val in[3] flags for pinn train name val out.*[0] same as input model file out.model[1] Trained PiNN model log file out.log[2] PiNN log (the evaluation errors) pinnMD The pinnMD process takes a trained model and runs a MD trajecotry, with some limited options regarding the dynamics. The process suppors PiNN models only for now. A list of models can be supplied to run an ensemble MD. For more complex processes, consider writing a customized MD process and include it in your workflow. Input Channels Channel Type i/o Note name val in[0] an id to identify the process model file in[1] trained PiNN model, can be a list of models init file in[2] initial geometry, in any ASE recognizable format flags val in[3] a string specifying the MD simulation see options below name val out.*[0] same as input traj file out.traj[1] output trajectory log file out.log[1] MD log Options The flags for pinnMD is specified in the form of --option1 val1 --option2 val , the available options and default valeus are listed below. Option Default Note --ensemble 'nvt' 'npt' or 'nvt' --T 273 Temperature in K --t 100 Time in ps --dt 0.5 Time step in fs --taut 100 Damping factor for thermostat in steps --taup 1000 Damping factor for barostat in steps --log-every 5 Log interval in steps --pressure 1 pressure in bar --compressibility 4.57e-5 compressibility in bar Source code nextflow . enable . dsl = 2 params . publish = 'pinn' process train { label 'pinn' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( dataset ), path ( input , stageAs: 'input' ), val ( flags ) output: tuple val ( name ), path ( 'model' , type: 'dir' ), emit: model tuple val ( name ), path ( 'pinn.log' ), emit: log script: convert_flag = \"${(flags =~ /--seed[\\s,\\=]\\d+/)[0]}\" train_flags = \"${flags.replaceAll(/--seed[\\s,\\=]\\d+/, '')}\" dataset = ( dataset instanceof Path ) ? dataset : dataset [ 0 ]. baseName + '.yml' \"\"\" #!/bin/bash pinn convert $dataset -o 'train:9,eval:1' $convert_flag if [ ! -f $input/params.yml ]; then mkdir -p model; cp $input model/params.yml else cp -rL $input model fi pinn train model/params.yml --model-dir='model'\\ --train-ds='train.yml' --eval-ds='eval.yml'\\ $train_flags pinn log model/eval > pinn.log \"\"\" } process md { label 'pinn' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( model , stageAs: 'model*' ), path ( init , stageAs: 'init*' ), val ( flags ) output: tuple val ( name ), path ( 'asemd.traj' ), emit: traj tuple val ( name ), path ( 'asemd.log' ), emit: log script: \"\"\" #!/usr/bin/env python import re import pinn import tensorflow as tf from ase import units from ase.io import read from ase.io.trajectory import Trajectory from ase.md import MDLogger from ase.md.velocitydistribution import MaxwellBoltzmannDistribution from ase.md.nptberendsen import NPTBerendsen from ase.md.nvtberendsen import NVTBerendsen from tips.bias import EnsembleBiasedCalculator # ------------ patch ase properties to write extra cols -------------------- from ase.calculators.calculator import all_properties all_properties+=[f'{prop}_{extra}' for prop in ['energy', 'forces', 'stress'] for extra in ['avg','std','bias']] # -------------------------------------------------------------------------- setup = { 'ensemble': 'nvt', # ensemble 'T': 340, # temperature in K 't': 50, # time in ps 'dt': 0.5, # timestep is fs 'taut': 100, # thermostat damping in steps 'taup': 1000, # barastat dampling in steps 'log-every': 20, # log interval in steps 'pressure': 1, # pressure in bar 'compressibility': 4.57e-4, # compressibility in bar^{-1} 'bias': None, 'kb': 0, 'sigma0': 0, } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) ensemble=setup['ensemble'] T=float(setup['T']) t=float(setup['t'])*units.fs*1e3 dt=float(setup['dt'])*units.fs taut=int(setup['taut']) taup=int(setup['taup']) every=int(setup['log-every']) pressure=float(setup['pressure']) compressibility=float(setup['compressibility']) ${(model instanceof Path) ? \"calc = pinn.get_calc('$model')\" : \"\"\" models = [ \"${model.join('\" , \"')}\" ] calcs = [ pinn . get_calc ( model ) for model in models ] if len ( calcs ) == 1 : calc = calcs [ 0 ] else : calc = EnsembleBiasedCalculator ( calcs , bias = setup [ 'bias' ], kb = float ( setup [ 'kb' ]), sigma0 = float ( setup [ 'sigma0' ])) \"\"\"} atoms = read(\"$init\") atoms.set_calculator(calc) if not atoms.has('momenta'): MaxwellBoltzmannDistribution(atoms, T*units.kB) if ensemble == 'npt': dyn = NPTBerendsen(atoms, timestep=dt, temperature=T, pressure=pressure, taut=dt * taut, taup=dt * taup, compressibility=compressibility) if ensemble == 'nvt': dyn = NVTBerendsen(atoms, timestep=dt, temperature=T, taut=dt * taut) dyn.attach( MDLogger(dyn, atoms, 'asemd.log',stress=True, mode=\"w\"), interval=int(every)) dyn.attach( Trajectory('asemd.traj', 'w', atoms).write, interval=int(every)) dyn.run(int(t/dt)) \"\"\" }","title":"PiNN"},{"location":"module/pinn/#pinn-workflows","text":"","title":"PiNN workflows"},{"location":"module/pinn/#pinntrain","text":"The pinnTrain process runs a training given a dataset and a PiNN input file. The input file can optionally be a model folder in such case the process contines training from the last checkpoint available. For list of flags available, see the PiNN documentation .","title":"pinnTrain"},{"location":"module/pinn/#input-channels","text":"Channel Type i/o Note name val in[0] an id to identify the process dataset file in[1] a dataset recognizable by PiNN input file in[2] a PiNN .yml input file flag val in[3] flags for pinn train name val out.*[0] same as input model file out.model[1] Trained PiNN model log file out.log[2] PiNN log (the evaluation errors)","title":"Input Channels"},{"location":"module/pinn/#pinnmd","text":"The pinnMD process takes a trained model and runs a MD trajecotry, with some limited options regarding the dynamics. The process suppors PiNN models only for now. A list of models can be supplied to run an ensemble MD. For more complex processes, consider writing a customized MD process and include it in your workflow.","title":"pinnMD"},{"location":"module/pinn/#input-channels_1","text":"Channel Type i/o Note name val in[0] an id to identify the process model file in[1] trained PiNN model, can be a list of models init file in[2] initial geometry, in any ASE recognizable format flags val in[3] a string specifying the MD simulation see options below name val out.*[0] same as input traj file out.traj[1] output trajectory log file out.log[1] MD log","title":"Input Channels"},{"location":"module/pinn/#options","text":"The flags for pinnMD is specified in the form of --option1 val1 --option2 val , the available options and default valeus are listed below. Option Default Note --ensemble 'nvt' 'npt' or 'nvt' --T 273 Temperature in K --t 100 Time in ps --dt 0.5 Time step in fs --taut 100 Damping factor for thermostat in steps --taup 1000 Damping factor for barostat in steps --log-every 5 Log interval in steps --pressure 1 pressure in bar --compressibility 4.57e-5 compressibility in bar Source code nextflow . enable . dsl = 2 params . publish = 'pinn' process train { label 'pinn' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( dataset ), path ( input , stageAs: 'input' ), val ( flags ) output: tuple val ( name ), path ( 'model' , type: 'dir' ), emit: model tuple val ( name ), path ( 'pinn.log' ), emit: log script: convert_flag = \"${(flags =~ /--seed[\\s,\\=]\\d+/)[0]}\" train_flags = \"${flags.replaceAll(/--seed[\\s,\\=]\\d+/, '')}\" dataset = ( dataset instanceof Path ) ? dataset : dataset [ 0 ]. baseName + '.yml' \"\"\" #!/bin/bash pinn convert $dataset -o 'train:9,eval:1' $convert_flag if [ ! -f $input/params.yml ]; then mkdir -p model; cp $input model/params.yml else cp -rL $input model fi pinn train model/params.yml --model-dir='model'\\ --train-ds='train.yml' --eval-ds='eval.yml'\\ $train_flags pinn log model/eval > pinn.log \"\"\" } process md { label 'pinn' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( model , stageAs: 'model*' ), path ( init , stageAs: 'init*' ), val ( flags ) output: tuple val ( name ), path ( 'asemd.traj' ), emit: traj tuple val ( name ), path ( 'asemd.log' ), emit: log script: \"\"\" #!/usr/bin/env python import re import pinn import tensorflow as tf from ase import units from ase.io import read from ase.io.trajectory import Trajectory from ase.md import MDLogger from ase.md.velocitydistribution import MaxwellBoltzmannDistribution from ase.md.nptberendsen import NPTBerendsen from ase.md.nvtberendsen import NVTBerendsen from tips.bias import EnsembleBiasedCalculator # ------------ patch ase properties to write extra cols -------------------- from ase.calculators.calculator import all_properties all_properties+=[f'{prop}_{extra}' for prop in ['energy', 'forces', 'stress'] for extra in ['avg','std','bias']] # -------------------------------------------------------------------------- setup = { 'ensemble': 'nvt', # ensemble 'T': 340, # temperature in K 't': 50, # time in ps 'dt': 0.5, # timestep is fs 'taut': 100, # thermostat damping in steps 'taup': 1000, # barastat dampling in steps 'log-every': 20, # log interval in steps 'pressure': 1, # pressure in bar 'compressibility': 4.57e-4, # compressibility in bar^{-1} 'bias': None, 'kb': 0, 'sigma0': 0, } flags = { k: v for k,v in re.findall('--(.*?)[\\\\s,\\\\=]([^\\\\s]*)', \"$flags\") } setup.update(flags) ensemble=setup['ensemble'] T=float(setup['T']) t=float(setup['t'])*units.fs*1e3 dt=float(setup['dt'])*units.fs taut=int(setup['taut']) taup=int(setup['taup']) every=int(setup['log-every']) pressure=float(setup['pressure']) compressibility=float(setup['compressibility']) ${(model instanceof Path) ? \"calc = pinn.get_calc('$model')\" : \"\"\" models = [ \"${model.join('\" , \"')}\" ] calcs = [ pinn . get_calc ( model ) for model in models ] if len ( calcs ) == 1 : calc = calcs [ 0 ] else : calc = EnsembleBiasedCalculator ( calcs , bias = setup [ 'bias' ], kb = float ( setup [ 'kb' ]), sigma0 = float ( setup [ 'sigma0' ])) \"\"\"} atoms = read(\"$init\") atoms.set_calculator(calc) if not atoms.has('momenta'): MaxwellBoltzmannDistribution(atoms, T*units.kB) if ensemble == 'npt': dyn = NPTBerendsen(atoms, timestep=dt, temperature=T, pressure=pressure, taut=dt * taut, taup=dt * taup, compressibility=compressibility) if ensemble == 'nvt': dyn = NVTBerendsen(atoms, timestep=dt, temperature=T, taut=dt * taut) dyn.attach( MDLogger(dyn, atoms, 'asemd.log',stress=True, mode=\"w\"), interval=int(every)) dyn.attach( Trajectory('asemd.traj', 'w', atoms).write, interval=int(every)) dyn.run(int(t/dt)) \"\"\" }","title":"Options"},{"location":"module/tips/","text":"TIPS module The tips.nf module contains several processes supplied by the TIPS library. convertDS The convertDS process converts a one dataset to another. The input/output formats are controlled by the flags channel. Channel specification Element Type i/o Note name val in[0] an id to identify the process input path in[1] input dataset flag val in[2] flags for tips convert name val out[0] same as input converted path out[1] converted dataset [ converted.* ] mergeDS The mergeDS process merges a number of single point calculations into one. Note that the process also expect a idx element in the input channel, which should give an index of the corresponding single point calculation, and will be saved into the merged.idx file. Channel specification Element Type i/o Note name val in[0] an id to identify the process idx val in[1] indices of single point simulations logs path in[2] logs from single point computations name val out[0] same as input idx path out[1] file that records the indices [ merged.idx ] merged path out[2] merged dataset [ merged.traj ] mixDS The mixDS process takes two datasets, called newDS and oldDS , and two flags newFlag and oldFlag , the datasets are first subsampled with corresponding flags, and them merged together. This process is mainly used to update a training set in an activated learning loop. Channel specification Element Type i/o Note name val in[0] an id to identify the process newDS path in[1] new dataset oldDS path in[2] old dataset newFlag path in[3] subsample flag for newDS oldFlag path in[4] subsample flag for oldDS name val out[0] same as input idx path out[1] merged index ( merged.idx ) checkConverge This workflow compares a sampled trajectories to labelled data. The output geometry will be: The last frame of the trajectory if the trajectory is deemed converted; The first frame of the trajectory otherwise. The convergence is controlled by the following parameters. Channel specification Element Type i/o Note name val in[0] an id to identify the process idx path in[1] index of labels in the trajectory label val in[2] labelled data set traj val in[3] sampled trajectory name val out[0] same as input geo path out[1] geometry out val out[2] a string of convergence information Parameters Parameter Default Description fmaxtol 2.0 Max error on forces emaxtol 0.02 Max error on energy frmsetol 0.15 Tolerance for force RMSE ermsetol 0.005 Tolerance for energy RMSE Source code nextflow . enable . dsl = 2 params . publish = \".\" def space_sep ( in ) {( in instanceof Path ) ? in : in . join ( ' ' )} process convert { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( in , stageAs: '.in*/*' ), val ( flags ) output: tuple val ( name ), path ( '*' ) script: \"\"\" tips convert ${space_sep(in)} $flags \"\"\" } process dsmix { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( newDS , stageAs: '*.traj' ), path ( oldDS , stageAs: 'old/*' ), val ( newFlag ), val ( oldFlag ) output: tuple val ( name ), path ( 'mix-ds.{tfr,yml}' ) script: \"\"\" tips convert old/${oldDS[0].baseName}.yml -f pinn -o old-ds -of asetraj $oldFlag tips convert ${space_sep(newDS)} -f asetraj -o tmp.traj -of asetraj tips convert tmp.traj -f asetraj -o new-ds -of asetraj $newFlag tips convert new-ds.traj old-ds.traj -f asetraj -o mix-ds -of pinn --shuffle $params.filters rm {new-ds,old-ds,tmp}.* \"\"\" } process merge { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), val ( idx ), path ( in , stageAs: '.in*/*' ), val ( flags ) output: tuple val ( name ), path ( 'merged.idx' ), path ( 'merged.traj' ) script: \"\"\" printf \"${idx.join('\\\\n')}\" > merged.idx tips convert ${space_sep(in)} -o merged -of asetraj $flags \"\"\" } process check { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( idx ), path ( logs ), path ( traj ) output: tuple val ( name ), path ( '*.xyz' ), stdout script: fmaxtol = params . fmaxtol emaxtol = params . emaxtol frmsetol = params . frmsetol ermsetol = params . ermsetol sp_points = params . sp_points \"\"\" #!/usr/bin/env python import numpy as np from ase import Atoms from ase.io import read, write from tips.io import load_ds from tips.io.filter import filters2fn filters = \"$params.filters\".replace(\"'\", '').split(' ')[1::2] filter_fn = filters2fn(filters) # ^ a crude extractor idx = [int(i) for i in np.loadtxt(\"$idx\")] logs = load_ds(\"$logs\", fmt='asetraj') traj = load_ds(\"$traj\", fmt='asetraj') idx, logs = tuple(zip(*( (i, datum) for (i, datum) in zip(idx, logs) if filter_fn(datum)))) e_label = np.array([datum['energy']/len(datum['elem']) for datum in logs]) f_label = np.array([datum['force'] for datum in logs]) e_pred = np.array([traj[i]['energy']/len(traj[i]['elem']) for i in idx]) f_pred = np.array([traj[i]['force'] for i in idx]) ecnt = np.sum(np.abs(e_pred-e_label)>$emaxtol) fcnt = np.sum(np.any(np.abs(f_pred-f_label)>$fmaxtol,axis=(1,2))) emax = np.max(np.abs(e_pred-e_label)) fmax = np.max(np.abs(f_pred-f_label)) ermse = np.sqrt(np.mean((e_pred-e_label)**2)) frmse = np.sqrt(np.mean((f_pred-f_label)**2)) converged = (emax<$emaxtol) and (fmax<$fmaxtol) and (ermse<$ermsetol) and (frmse<$frmsetol) and (len(idx)==$sp_points) geoname = \"$name\".split('/')[1] if converged: msg = f'Converged; will restart from latest frame.' new_geo = logs[np.argmax(idx)] else: msg = f'energy: {ecnt}/{len(idx)} failed, max={emax:.2f} rmse={ermse:.2f}; '\\ f'force: {fcnt}/{len(idx)} failed, max={fmax:.2f} rmse={frmse:.2f}.' new_geo = logs[np.argmin(idx)] atoms = Atoms(new_geo['elem'], positions=new_geo['coord'], cell=new_geo['cell'], pbc=True) write(f'{geoname}.xyz', atoms) print(msg) \"\"\" }","title":"TIPS"},{"location":"module/tips/#tips-module","text":"The tips.nf module contains several processes supplied by the TIPS library.","title":"TIPS module"},{"location":"module/tips/#convertds","text":"The convertDS process converts a one dataset to another. The input/output formats are controlled by the flags channel.","title":"convertDS"},{"location":"module/tips/#channel-specification","text":"Element Type i/o Note name val in[0] an id to identify the process input path in[1] input dataset flag val in[2] flags for tips convert name val out[0] same as input converted path out[1] converted dataset [ converted.* ]","title":"Channel specification"},{"location":"module/tips/#mergeds","text":"The mergeDS process merges a number of single point calculations into one. Note that the process also expect a idx element in the input channel, which should give an index of the corresponding single point calculation, and will be saved into the merged.idx file.","title":"mergeDS"},{"location":"module/tips/#channel-specification_1","text":"Element Type i/o Note name val in[0] an id to identify the process idx val in[1] indices of single point simulations logs path in[2] logs from single point computations name val out[0] same as input idx path out[1] file that records the indices [ merged.idx ] merged path out[2] merged dataset [ merged.traj ]","title":"Channel specification"},{"location":"module/tips/#mixds","text":"The mixDS process takes two datasets, called newDS and oldDS , and two flags newFlag and oldFlag , the datasets are first subsampled with corresponding flags, and them merged together. This process is mainly used to update a training set in an activated learning loop.","title":"mixDS"},{"location":"module/tips/#channel-specification_2","text":"Element Type i/o Note name val in[0] an id to identify the process newDS path in[1] new dataset oldDS path in[2] old dataset newFlag path in[3] subsample flag for newDS oldFlag path in[4] subsample flag for oldDS name val out[0] same as input idx path out[1] merged index ( merged.idx )","title":"Channel specification"},{"location":"module/tips/#checkconverge","text":"This workflow compares a sampled trajectories to labelled data. The output geometry will be: The last frame of the trajectory if the trajectory is deemed converted; The first frame of the trajectory otherwise. The convergence is controlled by the following parameters.","title":"checkConverge"},{"location":"module/tips/#channel-specification_3","text":"Element Type i/o Note name val in[0] an id to identify the process idx path in[1] index of labels in the trajectory label val in[2] labelled data set traj val in[3] sampled trajectory name val out[0] same as input geo path out[1] geometry out val out[2] a string of convergence information","title":"Channel specification"},{"location":"module/tips/#parameters","text":"Parameter Default Description fmaxtol 2.0 Max error on forces emaxtol 0.02 Max error on energy frmsetol 0.15 Tolerance for force RMSE ermsetol 0.005 Tolerance for energy RMSE Source code nextflow . enable . dsl = 2 params . publish = \".\" def space_sep ( in ) {( in instanceof Path ) ? in : in . join ( ' ' )} process convert { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( in , stageAs: '.in*/*' ), val ( flags ) output: tuple val ( name ), path ( '*' ) script: \"\"\" tips convert ${space_sep(in)} $flags \"\"\" } process dsmix { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( newDS , stageAs: '*.traj' ), path ( oldDS , stageAs: 'old/*' ), val ( newFlag ), val ( oldFlag ) output: tuple val ( name ), path ( 'mix-ds.{tfr,yml}' ) script: \"\"\" tips convert old/${oldDS[0].baseName}.yml -f pinn -o old-ds -of asetraj $oldFlag tips convert ${space_sep(newDS)} -f asetraj -o tmp.traj -of asetraj tips convert tmp.traj -f asetraj -o new-ds -of asetraj $newFlag tips convert new-ds.traj old-ds.traj -f asetraj -o mix-ds -of pinn --shuffle $params.filters rm {new-ds,old-ds,tmp}.* \"\"\" } process merge { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), val ( idx ), path ( in , stageAs: '.in*/*' ), val ( flags ) output: tuple val ( name ), path ( 'merged.idx' ), path ( 'merged.traj' ) script: \"\"\" printf \"${idx.join('\\\\n')}\" > merged.idx tips convert ${space_sep(in)} -o merged -of asetraj $flags \"\"\" } process check { label 'tips' publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( idx ), path ( logs ), path ( traj ) output: tuple val ( name ), path ( '*.xyz' ), stdout script: fmaxtol = params . fmaxtol emaxtol = params . emaxtol frmsetol = params . frmsetol ermsetol = params . ermsetol sp_points = params . sp_points \"\"\" #!/usr/bin/env python import numpy as np from ase import Atoms from ase.io import read, write from tips.io import load_ds from tips.io.filter import filters2fn filters = \"$params.filters\".replace(\"'\", '').split(' ')[1::2] filter_fn = filters2fn(filters) # ^ a crude extractor idx = [int(i) for i in np.loadtxt(\"$idx\")] logs = load_ds(\"$logs\", fmt='asetraj') traj = load_ds(\"$traj\", fmt='asetraj') idx, logs = tuple(zip(*( (i, datum) for (i, datum) in zip(idx, logs) if filter_fn(datum)))) e_label = np.array([datum['energy']/len(datum['elem']) for datum in logs]) f_label = np.array([datum['force'] for datum in logs]) e_pred = np.array([traj[i]['energy']/len(traj[i]['elem']) for i in idx]) f_pred = np.array([traj[i]['force'] for i in idx]) ecnt = np.sum(np.abs(e_pred-e_label)>$emaxtol) fcnt = np.sum(np.any(np.abs(f_pred-f_label)>$fmaxtol,axis=(1,2))) emax = np.max(np.abs(e_pred-e_label)) fmax = np.max(np.abs(f_pred-f_label)) ermse = np.sqrt(np.mean((e_pred-e_label)**2)) frmse = np.sqrt(np.mean((f_pred-f_label)**2)) converged = (emax<$emaxtol) and (fmax<$fmaxtol) and (ermse<$ermsetol) and (frmse<$frmsetol) and (len(idx)==$sp_points) geoname = \"$name\".split('/')[1] if converged: msg = f'Converged; will restart from latest frame.' new_geo = logs[np.argmax(idx)] else: msg = f'energy: {ecnt}/{len(idx)} failed, max={emax:.2f} rmse={ermse:.2f}; '\\ f'force: {fcnt}/{len(idx)} failed, max={fmax:.2f} rmse={frmse:.2f}.' new_geo = logs[np.argmin(idx)] atoms = Atoms(new_geo['elem'], positions=new_geo['coord'], cell=new_geo['cell'], pbc=True) write(f'{geoname}.xyz', atoms) print(msg) \"\"\" }","title":"Parameters"},{"location":"module/utils/","text":"Utility processes convert The convert process converts one dataset to another, where the dataset input can be a list of files (in such case), the dataset will be joined together. Channel Type Note (in) name val an id to identify the process (in) dataset file input dataset(s) (in) flags val tips convert options (out) output file the converted dataset","title":"Utility processes"},{"location":"module/utils/#utility-processes","text":"","title":"Utility processes"},{"location":"module/utils/#convert","text":"The convert process converts one dataset to another, where the dataset input can be a list of files (in such case), the dataset will be joined together. Channel Type Note (in) name val an id to identify the process (in) dataset file input dataset(s) (in) flags val tips convert options (out) output file the converted dataset","title":"convert"},{"location":"profiles/alvis/","text":"The Alvis Cluster The Alvis cluster is a national NAISS resource dedicated for Artificial Intelligence and Machine Learning research. The Alvis profile is configured to use the GPU resources there. TensorFlow and PiNN Since TensorFlow is already installed on Alvis, it's recommended to run PiNN with that. To do so, make a python environment with the supplied TF: ml TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1 python -m venv $HOME /pinn-tf26 source $HOME /pinn-tf26/bin/activate pip install git+https://github.com/teoroo-cmc/pinn.git The above creates a python virtual environment based on the system TF module. When you need to run PiNN manually in a new bash session, you need to load the module and activate the environment: ml TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1 source $HOME /pinn-tf26/bin/activate You might also want to make this enivronment avaialble to the Alvis OnDemand portal, following the instruction (after activating your environment): pip install ipykernel python -m ipykernel install --user --name pinn-tf26 --display-name \"pinn-tf26\" CP2K The container image in NGC for CP2K supports acceleration through CUDA. You will need to build the singularity file following the NGC instructions, and point the profile to your image. The accelerators should be picked up automatically, for which you can verify by looking for the ACC: tags in the CP2K log file. Profile profiles { alvis { params { cp2k_cmd = 'OMP_NUM_THREADS=2 mpirun -n 4 cp2k.psmp' } executor { name = 'slurm' queueSize = 100 submitRateLimit = '120 min' } process { time = '3d' executor = 'slurm' errorStrategy = 'ignore' withLabel: 'tips|pinn' { beforeScript = 'source $HOME/pinn-tf26/bin/activate' module = 'TensorFlow/2.6.0-fosscuda-2021a-CUDA-11.3.1' } withLabel: 'tips|molutils' { executor = 'local' } withLabel: 'pinn' { scratch = true clusterOptions = '--gres=gpu:T4:1' container = 'teoroo/pinnacle:pinn' } withLabel: 'cp2k' { scratch = true clusterOptions = '--gres=gpu:T4:2' container = 'nvcr.io/hpc/cp2k:v9.1.0' } } } }","title":"Alvis"},{"location":"profiles/alvis/#the-alvis-cluster","text":"The Alvis cluster is a national NAISS resource dedicated for Artificial Intelligence and Machine Learning research. The Alvis profile is configured to use the GPU resources there.","title":"The Alvis Cluster"},{"location":"profiles/alvis/#tensorflow-and-pinn","text":"Since TensorFlow is already installed on Alvis, it's recommended to run PiNN with that. To do so, make a python environment with the supplied TF: ml TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1 python -m venv $HOME /pinn-tf26 source $HOME /pinn-tf26/bin/activate pip install git+https://github.com/teoroo-cmc/pinn.git The above creates a python virtual environment based on the system TF module. When you need to run PiNN manually in a new bash session, you need to load the module and activate the environment: ml TensorFlow/2.6.0-foss-2021a-CUDA-11.3.1 source $HOME /pinn-tf26/bin/activate You might also want to make this enivronment avaialble to the Alvis OnDemand portal, following the instruction (after activating your environment): pip install ipykernel python -m ipykernel install --user --name pinn-tf26 --display-name \"pinn-tf26\"","title":"TensorFlow and PiNN"},{"location":"profiles/alvis/#cp2k","text":"The container image in NGC for CP2K supports acceleration through CUDA. You will need to build the singularity file following the NGC instructions, and point the profile to your image. The accelerators should be picked up automatically, for which you can verify by looking for the ACC: tags in the CP2K log file.","title":"CP2K"},{"location":"profiles/alvis/#profile","text":"profiles { alvis { params { cp2k_cmd = 'OMP_NUM_THREADS=2 mpirun -n 4 cp2k.psmp' } executor { name = 'slurm' queueSize = 100 submitRateLimit = '120 min' } process { time = '3d' executor = 'slurm' errorStrategy = 'ignore' withLabel: 'tips|pinn' { beforeScript = 'source $HOME/pinn-tf26/bin/activate' module = 'TensorFlow/2.6.0-fosscuda-2021a-CUDA-11.3.1' } withLabel: 'tips|molutils' { executor = 'local' } withLabel: 'pinn' { scratch = true clusterOptions = '--gres=gpu:T4:1' container = 'teoroo/pinnacle:pinn' } withLabel: 'cp2k' { scratch = true clusterOptions = '--gres=gpu:T4:2' container = 'nvcr.io/hpc/cp2k:v9.1.0' } } } }","title":"Profile"},{"location":"profiles/overview/","text":"Profiles for PiNNAcLe Workflows This part of the documentation hosts the setups to run PiNNAcLe workflows on different computational resources. The instructions also include the installation of libraries to run optimally on specific resource. Please note that the softwares installed on supercomputer are subject to changes, and the developers are not tracking all of them. You are nevertheless welcome to report deprecated instructions or update them. Standard profile profiles { standard { executor { name = 'local' cpus = 8 } params { cp2k_cmd = 'source /opt/cp2k/prod_entrypoint.sh local popt; cp2k.popt' lmp_cmd = 'lmp_mpi' } env { OMP_NUM_THREADS = '1' } process { cpus = 1 withLabel: \"pinn|tips\" { container = 'teoroo/pinnacle:pinn' } withLabel: cp2k { container = 'nvcr.io/hpc/cp2k:v9.1.0' } withLabel: dftb { container = 'teoroo/pinnacle:dftb' } withLabel: lammps { container = 'lammps/lammps:stable_7Aug2019_ubuntu18.04_openmpi_py3' } withLabel: molutils { container = 'teoroo/pinnacle:molutils' } } singularity { enabled = true } } } The standard profiles uses container images for its processes, most of them are compiled hosted on Dockerhub and automatically built (with a few exceptions). Below are a list of available softwares and links to the docker image, where the build scripts are also kept. Software Version Dockerhub image packmol 20.14.2 teoroo/pinnacle:molutils molptemlate 2.20.19 teoroo/pinnacle:molutils awk, bc, openbabel latest (Debian) teoroo/pinnacle:molutils dftbplus 22.2 teoroo/pinnacle:dftbplus tips latest teoroo/pinnacle:pinn pinn latest teoroo/pinnacle:pinn cp2k 9.1.0 (NGC) nvcr.io/hpc/cp2k:v9.1.0 lammps 29Sep2021 lammps/lammps:stable_29Sep2021_centos7_openmpi_py3","title":"Overview"},{"location":"profiles/overview/#profiles-for-pinnacle-workflows","text":"This part of the documentation hosts the setups to run PiNNAcLe workflows on different computational resources. The instructions also include the installation of libraries to run optimally on specific resource. Please note that the softwares installed on supercomputer are subject to changes, and the developers are not tracking all of them. You are nevertheless welcome to report deprecated instructions or update them.","title":"Profiles for PiNNAcLe Workflows"},{"location":"profiles/overview/#standard-profile","text":"profiles { standard { executor { name = 'local' cpus = 8 } params { cp2k_cmd = 'source /opt/cp2k/prod_entrypoint.sh local popt; cp2k.popt' lmp_cmd = 'lmp_mpi' } env { OMP_NUM_THREADS = '1' } process { cpus = 1 withLabel: \"pinn|tips\" { container = 'teoroo/pinnacle:pinn' } withLabel: cp2k { container = 'nvcr.io/hpc/cp2k:v9.1.0' } withLabel: dftb { container = 'teoroo/pinnacle:dftb' } withLabel: lammps { container = 'lammps/lammps:stable_7Aug2019_ubuntu18.04_openmpi_py3' } withLabel: molutils { container = 'teoroo/pinnacle:molutils' } } singularity { enabled = true } } } The standard profiles uses container images for its processes, most of them are compiled hosted on Dockerhub and automatically built (with a few exceptions). Below are a list of available softwares and links to the docker image, where the build scripts are also kept. Software Version Dockerhub image packmol 20.14.2 teoroo/pinnacle:molutils molptemlate 2.20.19 teoroo/pinnacle:molutils awk, bc, openbabel latest (Debian) teoroo/pinnacle:molutils dftbplus 22.2 teoroo/pinnacle:dftbplus tips latest teoroo/pinnacle:pinn pinn latest teoroo/pinnacle:pinn cp2k 9.1.0 (NGC) nvcr.io/hpc/cp2k:v9.1.0 lammps 29Sep2021 lammps/lammps:stable_29Sep2021_centos7_openmpi_py3","title":"Standard profile"},{"location":"profiles/teoroo2/","text":"The Teoroo2 Cluster Note for scheduling GPU jobs on Teoroo2 The official nextflow does not support scheduling GPU resources on a local executor. On Teoroo, a custom nextflow build is avialble. Use /sw/nf instead of nextflow , and set the GPUs available to a workflow with CUDA_VISIALBE_DEVICES , namely: export CUDA_VISIBLE_DEVICES = 1 ,2,3,4 /sw/nf main.nf -entry h2o_demo -profile teoroo2 The Teoroo2 profiel use containerized runtimes as the standard profile , the difference is that local copies of the images are available and used directly. Local singularity images in sif format are stored in the /sw/pinnacle folder. All processes are configured to run on single thread, and the pinn-based jobs are configured to run with one GPU. Profile profiles { teoroo2 { executor { name = 'local' cpus = 32 } params { cp2k_cmd = 'source /opt/cp2k/prod_entrypoint.sh local popt; cp2k.popt' lmp_cmd = 'lmp_mpi' } env { OMP_NUM_THREADS = '1' } process { cpus = 1 accelerator = 0 withLabel: pinn { accelerator = 1 } withLabel: \"pinn|tips\" { container = '/sw/pinnacle/pinn.sif' } withLabel: cp2k { container = '/sw/pinnacle/cp2k.sif' } withLabel: dftb { container = '/sw/pinnacle/dftb.sif' } withLabel: lammps { container = '/sw/pinnacle/lammps.sif' } withLabel: molutils { container = '/sw/pinnacle/molutils.sif' } } singularity { enabled = true runOptions = '--nv' } } }","title":"Teoroo2"},{"location":"profiles/teoroo2/#the-teoroo2-cluster","text":"Note for scheduling GPU jobs on Teoroo2 The official nextflow does not support scheduling GPU resources on a local executor. On Teoroo, a custom nextflow build is avialble. Use /sw/nf instead of nextflow , and set the GPUs available to a workflow with CUDA_VISIALBE_DEVICES , namely: export CUDA_VISIBLE_DEVICES = 1 ,2,3,4 /sw/nf main.nf -entry h2o_demo -profile teoroo2 The Teoroo2 profiel use containerized runtimes as the standard profile , the difference is that local copies of the images are available and used directly. Local singularity images in sif format are stored in the /sw/pinnacle folder. All processes are configured to run on single thread, and the pinn-based jobs are configured to run with one GPU.","title":"The Teoroo2 Cluster"},{"location":"profiles/teoroo2/#profile","text":"profiles { teoroo2 { executor { name = 'local' cpus = 32 } params { cp2k_cmd = 'source /opt/cp2k/prod_entrypoint.sh local popt; cp2k.popt' lmp_cmd = 'lmp_mpi' } env { OMP_NUM_THREADS = '1' } process { cpus = 1 accelerator = 0 withLabel: pinn { accelerator = 1 } withLabel: \"pinn|tips\" { container = '/sw/pinnacle/pinn.sif' } withLabel: cp2k { container = '/sw/pinnacle/cp2k.sif' } withLabel: dftb { container = '/sw/pinnacle/dftb.sif' } withLabel: lammps { container = '/sw/pinnacle/lammps.sif' } withLabel: molutils { container = '/sw/pinnacle/molutils.sif' } } singularity { enabled = true runOptions = '--nv' } } }","title":"Profile"},{"location":"recipe/acle/","text":"Activated Learning The activated learning recipe actively samples a potential energy surface. The workflow is controlled by several subworkflows, including the training, sampling, and labelling processes. The overall workflow is shown in the following diagram: The workflow can be used as either an entrypoint or a subworkflow. Some parameters that set up the initial datasets and models are taken by the entrypoint only, see below for usage and tables of parameters. Entrypoint Convert initial dataset and geometry from a ASE trajectory As a example, consider a trajectory file readable by ASE as input, the tips convert CLI tool can be used get the initial dataset and geometries: # generating my_ds.{yml,tfr} tips convert -f asetraj data.traj -of pinn my_ds # generating the initial geometry tips convert -f asetraj data.traj --subsample uniform -of idx.xyz To run the workflow as an entrypoint ( single quotes are necessary ): nextflow run main.nf -entry acle --init_ds 'myds.{yml,tfr}' --init_geo '*.xyz' ... It is possible to restart a project from a ceratin generation, while keeping the folder structure: nextflow run main.nf -entry acle --restart_from 30 --restart_conv true When restarted in the above way the init_* parameters will be ignored. This method is mainly for small changes of the sampled ensemble, e.g., an ad hook change of temperature. In cases where this is not enough, it is advisable to rerun the workflow under a different --proj . Parameters Parameter Description Default init_geo inital geometries for sampling input/geo/*.xyz init_model initial model or model parameters input/pinn/pinet-adam.yml init_ds initial dataset input/ds/init-ds.{yml,tfr} init_time sampling time scale in ps 1.0 init_steps training steps for initial model 100000 restart_from restart from a given generation false restart_conv restart from a converged model (model will be retrained if false ) false Subworkflow Input/Output Channels Channel I/O[idx] Type Description gen in[0] val generation of the model geo in[1] file initial geometry ds in[2] file training dataset steps in[3] val training steps time in[4] val sampling timescale converge in[5] val whether the input model is deemed converged The AcLe subworkflow is a recursive workflow, and the input and output shares the same data structure. Parameters Parameter Description Default proj folder for storing results acle ref reference calculation module dftb ref_inp reference input file input/dftb/xtb.py mpl machine learning potential module pinn train_flags mlp training flags --log-every 10000 --ckpt-every 100000 --batch 1 --max-ckpts 1 --shuffle buffter 3000 train_init mlp training flags --init max_gen maximal number of generations 40 min_time minimal timescale for sampling 1.0 max_time maximal timescale for sampling 1000.0 md_flags flags for md sampling, see ase module for details --ensemble nvt --dt 0.5 --log-every 100 --T 340 collect_flags collection flags for the data to label -f asetraj --subsample uniform --nsample 10 -of idx.xyz -o ds sp_points number of single points per sampled trajectory 10 old_flag selection rule for the old dataset --nsample 2700 new_flag selection rule for the new dataset --nsample 300 sp_points number of single points for each sampled trajectory 50 emaxtol toleranace for max error error 0.020 ermsetol toleranace for energy RMSE 0.005 fmaxtol toleranace for max force (component) error 0.800 frmsetol toleranace for force (component) RMSE 0.200 retrain_step number of retrain steps per generation 100000 acc_fac factor to acceralate the sampling 2.0 brake_fac factor to slow down the sampling 1.0 Source Code: nextflow/acle.nf #!/usr/bin/env nextflow // The activated learning workflow ====================================================== // // The '--proj' parameter controls the output directory. See the parameters // sections below for other parameters that can be tuned for the workflow. // // written by Yunqi Shao, first ver.: 2022.Aug.29 // adapted as PiNNAcLe recipe: 2023.Apr.24 //======================================================================================== nextflow . enable . dsl = 2 nextflow . preview . recursion = true def logger ( msg ) { logfile = file ( \"$params.publish/pinnacle.log\" ) if (! logfile . getParent (). exists ()) { logfile . getParent (). mkdirs ()} logfile . append ( \"$msg \\n\" ) } // entrypoint parameters ================================================================== params . publish = 'acle' params . init_geo = 'input/geo/*.xyz' params . init_model = 'input/pinn/pinet-adam.yml' params . init_ds = 'input/dataset/init-ds.{yml,tfr}' params . init_time = 0.5 params . init_steps = 200000 params . ens_size = 1 params . restart_from = false params . restart_conv = false //======================================================================================== // acle parameters ======================================================================= params . ref = 'dftb' // reference (module name) params . ref_inp = 'input/dftb/xtb.py' params . mpl = 'pinn' // machine learning potential (module name) params . train_flags = '--log-every 10000 --ckpt-every 100000 --batch 1 --max-ckpts 1 --shuffle-buffer 3000' params . train_init = '--init' params . exit_at_max_time = false params . max_gen = 40 params . min_time = 0.5 params . max_time = 1000.0 params . md_flags = '--ensemble nvt --dt 0.5 --log-every 100 --T 340' params . collect_flags = '-f asetraj --subsample uniform --nsample 10 -of idx.xyz -o ds' params . sp_points = 10 params . merge_flags = '-f asetraj' params . old_flag = '--nsample 240' params . new_flag = '--psample 100' params . frmsetol = 0.150 params . ermsetol = 0.005 params . fmaxtol = 2.000 params . emaxtol = 0.020 params . retrain_step = 100000 params . acc_fac = 4.0 params . brake_fac = 1.0 //======================================================================================== // Imports (publish directories are set here) ============================================ include { convert } from './module/tips.nf' addParams ( publish: \"$params.publish/collect\" ) include { dsmix } from './module/tips.nf' addParams ( publish: \"$params.publish/dsmix\" ) include { merge } from './module/tips.nf' addParams ( publish: \"$params.publish/merge\" ) include { check } from './module/tips.nf' addParams ( publish: \"$params.publish/check\" ) include { train } from \"./module/${params.mpl}.nf\" addParams ( publish: \"$params.publish/models\" ) include { md } from \"./module/${params.mpl}.nf\" addParams ( publish: \"$params.publish/md\" ) include { sp } from \"./module/${params.ref}.nf\" addParams ( publish: \"$params.publish/label\" ) //======================================================================================== // Entry point workflow entry { logger ( 'Starting an AcLe Loop' ) init_ds = file ( params . init_ds ) init_geo = file ( params . init_geo ) params . geo_size = init_geo . size ens_size = params . ens_size . toInteger () logger ( \"Initial dataset: ${init_ds.name};\" ) logger ( \"Initial geometries ($params.geo_size) in ${params.init_geo}\" ) if ( params . restart_from ) { init_gen = params . restart_from . toString () init_models = file ( \"${params.publish}/models/gen${init_gen}/*/model\" , type: 'dir' ) init_geo = file ( \"${params.publish}/check/gen${init_gen}/*/*.xyz\" ) init_ds = file ( \"${params.publish}/dsmix/${init_gen}/mix-ds.{yml,tfr}\" ) logger ( \"restarting from gen$init_gen ensemble of size $ens_size;\" ) init_gen = ( init_gen . toInteger ()+ 1 ). toString () } else { init_gen = '0' init_models = file ( params . init_model , type: 'any' ) if (!( init_models instanceof Path )) { logger ( \"restarting from an ensemble of size $ens_size;\" ) } else { init_models = [ init_models ] * ens_size logger ( \"starting from scratch with the input $init_models.name of size $ens_size;\" ) } } assert ens_size == init_models . size : \"ens_size ($ens_size) does not match input ($init_models.size)\" steps = params . init_steps . toInteger () time = params . init_time . toFloat () converge = params . restart_conv . toBoolean () init_inp = [ init_gen , init_geo , init_ds , init_models , steps , time , converge ] ch_inp = Channel . value ( init_inp ) acle ( ch_inp ) } // Main Iteration and Loops ============================================================== workflow acle { take: ch_init main: loop . recurse ( ch_init ) . until { it [ 0 ]. toInteger ()> params . max_gen || ( it [ 5 ]>= params . max_time . toFloat () && params . exit_at_max_time ) } } // Loop for each iteration ================================================================= workflow loop { take: ch_inp main: // retrain or keep the model ============================================================ ch_inp \\ | branch { gen , geo , ds , models , step , time , converge -> \\ keep: converge return [ gen , models ] retrain: ! converge return [ gen , models , ds , ( 1 .. params . ens_size ). toList (), step ]} \\ | set { ch_model } ch_model . retrain . transpose ( by: [ 1 , 3 ]) \\ | map { gen , model , ds , seed , steps -> \\ [ \"gen$gen/model$seed\" , ds , model , params . train_flags + \" --seed $seed --train-steps $steps\" + ( gen . toInteger ()== 1 ? \" $params.train_init\" : '' )]} \\ | train train . out . model \\ | map { name , model -> ( name =~ /gen(\\d+)\\/model(\\d+)/ )[ 0 ][ 1 , 2 ]+[ model ]} \\ | map { gen , seed , model -> [ gen , model ]} \\ | mix ( ch_model . keep . transpose ()) \\ | groupTuple ( size: params . ens_size ) \\ | set { nx_models } //======================================================================================= // sampling with ensable NN ============================================================= ch_inp | map {[ it [ 0 ], it [ 1 ], it [ 5 ]]} | transpose | set { ch_init_t } // init and time nx_models \\ | combine ( ch_init_t , by: 0 ) \\ | map { gen , models , init , t -> \\ [ \"gen$gen/$init.baseName\" , models , init , params . md_flags + \" --t $t\" ]} \\ | md md . out . traj . set { ch_trajs } //======================================================================================= // relabel with reference =============================================================== ref_inp = file ( params . ref_inp ) ch_trajs \\ | map { name , traj -> [ name , traj , params . collect_flags ]} \\ | convert \\ | flatMap { name , inps -> inps . collect {[ \"$name/$it.baseName\" , it ]}} \\ | map { name , geo -> [ name , ref_inp , geo ]} \\ | sp sp . out \\ | map { name , logs -> ( name =~ /(gen\\d+\\/.+)\\/(\\d+)/ )[ 0 ][ 1 , 2 ]+[ logs ]} \\ | map { name , idx , logs -> [ name , idx . toInteger (), logs ]} \\ | groupTuple ( size: params . sp_points ) \\ | map { name , idx , logs -> [ name , idx , logs , params . merge_flags ]} \\ | merge \\ | set { ch_new_ds } //======================================================================================= // check convergence ==================================================================== ch_new_ds \\ | join ( ch_trajs ) \\ | check \\ check . out \\ | map { name , geo , msg -> \\ [( name =~ /gen(\\d+)\\/.+/ )[ 0 ][ 1 ], geo , msg . contains ( 'Converged' )]} \\ | groupTuple ( size: params . geo_size . toInteger ()) \\ | map { gen , geo , conv -> [ gen , geo , conv . every ()]} | set { nx_geo_converge } //======================================================================================= // mix the new dataset ================================================================== ch_inp . map {[ it [ 0 ], it [ 2 ]]}. set { ch_old_ds } ch_new_ds \\ | map { name , idx , ds -> [( name =~ /gen(\\d+)\\/.+/ )[ 0 ][ 1 ], ds ]} \\ | groupTuple ( size: params . geo_size . toInteger ()) \\ | join ( ch_old_ds ) \\ | map { it +[ params . new_flag , params . old_flag ]} \\ | dsmix \\ | set { nx_ds } //======================================================================================= // combine everything for new inputs ==================================================== ch_inp . map {[ it [ 0 ], it [ 4 ]]}. set { nx_step } ch_inp . map {[ it [ 0 ], it [ 5 ]]}. set { nx_time } acc_fac = params . acc_fac . toFloat () brake_fac = params . brake_fac . toFloat () min_time = params . min_time . toFloat () max_time = params . max_time . toFloat () retrain_step = params . retrain_step . toInteger () nx_geo_converge | join ( nx_models ) | join ( nx_ds ) | join ( nx_time ) | join ( nx_step ) \\ | map { gen , geo , converge , models , ds , time , step -> \\ [( gen . toInteger ()+ 1 ). toString (), geo , ds , models , \\ converge ? step : step + retrain_step , \\ converge ? Math . min ( time * acc_fac , max_time ) : Math . max ( time * brake_fac , min_time ), \\ converge ]} \\ | set { nx_inp } //======================================================================================= ch_inp . subscribe { logger ( \"[gen${it[0]}] ${it[-1]? 'not training': 'training'} the models.\" )} check . out . subscribe { name , geo , msg -> logger ( \"[$name] ${msg.trim()}\" )} nx_inp . subscribe { logger ( '-' * 80 + '\\n' + \"[gen${it[0]}] next time scale ${it[5]} ps, ${it[6] ? 'no training planned' : 'next training step '+it[4]}.\" ) } \\ emit: nx_inp }","title":"AcLe"},{"location":"recipe/acle/#activated-learning","text":"The activated learning recipe actively samples a potential energy surface. The workflow is controlled by several subworkflows, including the training, sampling, and labelling processes. The overall workflow is shown in the following diagram: The workflow can be used as either an entrypoint or a subworkflow. Some parameters that set up the initial datasets and models are taken by the entrypoint only, see below for usage and tables of parameters.","title":"Activated Learning"},{"location":"recipe/acle/#entrypoint","text":"Convert initial dataset and geometry from a ASE trajectory As a example, consider a trajectory file readable by ASE as input, the tips convert CLI tool can be used get the initial dataset and geometries: # generating my_ds.{yml,tfr} tips convert -f asetraj data.traj -of pinn my_ds # generating the initial geometry tips convert -f asetraj data.traj --subsample uniform -of idx.xyz To run the workflow as an entrypoint ( single quotes are necessary ): nextflow run main.nf -entry acle --init_ds 'myds.{yml,tfr}' --init_geo '*.xyz' ... It is possible to restart a project from a ceratin generation, while keeping the folder structure: nextflow run main.nf -entry acle --restart_from 30 --restart_conv true When restarted in the above way the init_* parameters will be ignored. This method is mainly for small changes of the sampled ensemble, e.g., an ad hook change of temperature. In cases where this is not enough, it is advisable to rerun the workflow under a different --proj .","title":"Entrypoint"},{"location":"recipe/acle/#parameters","text":"Parameter Description Default init_geo inital geometries for sampling input/geo/*.xyz init_model initial model or model parameters input/pinn/pinet-adam.yml init_ds initial dataset input/ds/init-ds.{yml,tfr} init_time sampling time scale in ps 1.0 init_steps training steps for initial model 100000 restart_from restart from a given generation false restart_conv restart from a converged model (model will be retrained if false ) false","title":"Parameters"},{"location":"recipe/acle/#subworkflow","text":"","title":"Subworkflow"},{"location":"recipe/acle/#inputoutput-channels","text":"Channel I/O[idx] Type Description gen in[0] val generation of the model geo in[1] file initial geometry ds in[2] file training dataset steps in[3] val training steps time in[4] val sampling timescale converge in[5] val whether the input model is deemed converged The AcLe subworkflow is a recursive workflow, and the input and output shares the same data structure.","title":"Input/Output Channels"},{"location":"recipe/acle/#parameters_1","text":"Parameter Description Default proj folder for storing results acle ref reference calculation module dftb ref_inp reference input file input/dftb/xtb.py mpl machine learning potential module pinn train_flags mlp training flags --log-every 10000 --ckpt-every 100000 --batch 1 --max-ckpts 1 --shuffle buffter 3000 train_init mlp training flags --init max_gen maximal number of generations 40 min_time minimal timescale for sampling 1.0 max_time maximal timescale for sampling 1000.0 md_flags flags for md sampling, see ase module for details --ensemble nvt --dt 0.5 --log-every 100 --T 340 collect_flags collection flags for the data to label -f asetraj --subsample uniform --nsample 10 -of idx.xyz -o ds sp_points number of single points per sampled trajectory 10 old_flag selection rule for the old dataset --nsample 2700 new_flag selection rule for the new dataset --nsample 300 sp_points number of single points for each sampled trajectory 50 emaxtol toleranace for max error error 0.020 ermsetol toleranace for energy RMSE 0.005 fmaxtol toleranace for max force (component) error 0.800 frmsetol toleranace for force (component) RMSE 0.200 retrain_step number of retrain steps per generation 100000 acc_fac factor to acceralate the sampling 2.0 brake_fac factor to slow down the sampling 1.0 Source Code: nextflow/acle.nf #!/usr/bin/env nextflow // The activated learning workflow ====================================================== // // The '--proj' parameter controls the output directory. See the parameters // sections below for other parameters that can be tuned for the workflow. // // written by Yunqi Shao, first ver.: 2022.Aug.29 // adapted as PiNNAcLe recipe: 2023.Apr.24 //======================================================================================== nextflow . enable . dsl = 2 nextflow . preview . recursion = true def logger ( msg ) { logfile = file ( \"$params.publish/pinnacle.log\" ) if (! logfile . getParent (). exists ()) { logfile . getParent (). mkdirs ()} logfile . append ( \"$msg \\n\" ) } // entrypoint parameters ================================================================== params . publish = 'acle' params . init_geo = 'input/geo/*.xyz' params . init_model = 'input/pinn/pinet-adam.yml' params . init_ds = 'input/dataset/init-ds.{yml,tfr}' params . init_time = 0.5 params . init_steps = 200000 params . ens_size = 1 params . restart_from = false params . restart_conv = false //======================================================================================== // acle parameters ======================================================================= params . ref = 'dftb' // reference (module name) params . ref_inp = 'input/dftb/xtb.py' params . mpl = 'pinn' // machine learning potential (module name) params . train_flags = '--log-every 10000 --ckpt-every 100000 --batch 1 --max-ckpts 1 --shuffle-buffer 3000' params . train_init = '--init' params . exit_at_max_time = false params . max_gen = 40 params . min_time = 0.5 params . max_time = 1000.0 params . md_flags = '--ensemble nvt --dt 0.5 --log-every 100 --T 340' params . collect_flags = '-f asetraj --subsample uniform --nsample 10 -of idx.xyz -o ds' params . sp_points = 10 params . merge_flags = '-f asetraj' params . old_flag = '--nsample 240' params . new_flag = '--psample 100' params . frmsetol = 0.150 params . ermsetol = 0.005 params . fmaxtol = 2.000 params . emaxtol = 0.020 params . retrain_step = 100000 params . acc_fac = 4.0 params . brake_fac = 1.0 //======================================================================================== // Imports (publish directories are set here) ============================================ include { convert } from './module/tips.nf' addParams ( publish: \"$params.publish/collect\" ) include { dsmix } from './module/tips.nf' addParams ( publish: \"$params.publish/dsmix\" ) include { merge } from './module/tips.nf' addParams ( publish: \"$params.publish/merge\" ) include { check } from './module/tips.nf' addParams ( publish: \"$params.publish/check\" ) include { train } from \"./module/${params.mpl}.nf\" addParams ( publish: \"$params.publish/models\" ) include { md } from \"./module/${params.mpl}.nf\" addParams ( publish: \"$params.publish/md\" ) include { sp } from \"./module/${params.ref}.nf\" addParams ( publish: \"$params.publish/label\" ) //======================================================================================== // Entry point workflow entry { logger ( 'Starting an AcLe Loop' ) init_ds = file ( params . init_ds ) init_geo = file ( params . init_geo ) params . geo_size = init_geo . size ens_size = params . ens_size . toInteger () logger ( \"Initial dataset: ${init_ds.name};\" ) logger ( \"Initial geometries ($params.geo_size) in ${params.init_geo}\" ) if ( params . restart_from ) { init_gen = params . restart_from . toString () init_models = file ( \"${params.publish}/models/gen${init_gen}/*/model\" , type: 'dir' ) init_geo = file ( \"${params.publish}/check/gen${init_gen}/*/*.xyz\" ) init_ds = file ( \"${params.publish}/dsmix/${init_gen}/mix-ds.{yml,tfr}\" ) logger ( \"restarting from gen$init_gen ensemble of size $ens_size;\" ) init_gen = ( init_gen . toInteger ()+ 1 ). toString () } else { init_gen = '0' init_models = file ( params . init_model , type: 'any' ) if (!( init_models instanceof Path )) { logger ( \"restarting from an ensemble of size $ens_size;\" ) } else { init_models = [ init_models ] * ens_size logger ( \"starting from scratch with the input $init_models.name of size $ens_size;\" ) } } assert ens_size == init_models . size : \"ens_size ($ens_size) does not match input ($init_models.size)\" steps = params . init_steps . toInteger () time = params . init_time . toFloat () converge = params . restart_conv . toBoolean () init_inp = [ init_gen , init_geo , init_ds , init_models , steps , time , converge ] ch_inp = Channel . value ( init_inp ) acle ( ch_inp ) } // Main Iteration and Loops ============================================================== workflow acle { take: ch_init main: loop . recurse ( ch_init ) . until { it [ 0 ]. toInteger ()> params . max_gen || ( it [ 5 ]>= params . max_time . toFloat () && params . exit_at_max_time ) } } // Loop for each iteration ================================================================= workflow loop { take: ch_inp main: // retrain or keep the model ============================================================ ch_inp \\ | branch { gen , geo , ds , models , step , time , converge -> \\ keep: converge return [ gen , models ] retrain: ! converge return [ gen , models , ds , ( 1 .. params . ens_size ). toList (), step ]} \\ | set { ch_model } ch_model . retrain . transpose ( by: [ 1 , 3 ]) \\ | map { gen , model , ds , seed , steps -> \\ [ \"gen$gen/model$seed\" , ds , model , params . train_flags + \" --seed $seed --train-steps $steps\" + ( gen . toInteger ()== 1 ? \" $params.train_init\" : '' )]} \\ | train train . out . model \\ | map { name , model -> ( name =~ /gen(\\d+)\\/model(\\d+)/ )[ 0 ][ 1 , 2 ]+[ model ]} \\ | map { gen , seed , model -> [ gen , model ]} \\ | mix ( ch_model . keep . transpose ()) \\ | groupTuple ( size: params . ens_size ) \\ | set { nx_models } //======================================================================================= // sampling with ensable NN ============================================================= ch_inp | map {[ it [ 0 ], it [ 1 ], it [ 5 ]]} | transpose | set { ch_init_t } // init and time nx_models \\ | combine ( ch_init_t , by: 0 ) \\ | map { gen , models , init , t -> \\ [ \"gen$gen/$init.baseName\" , models , init , params . md_flags + \" --t $t\" ]} \\ | md md . out . traj . set { ch_trajs } //======================================================================================= // relabel with reference =============================================================== ref_inp = file ( params . ref_inp ) ch_trajs \\ | map { name , traj -> [ name , traj , params . collect_flags ]} \\ | convert \\ | flatMap { name , inps -> inps . collect {[ \"$name/$it.baseName\" , it ]}} \\ | map { name , geo -> [ name , ref_inp , geo ]} \\ | sp sp . out \\ | map { name , logs -> ( name =~ /(gen\\d+\\/.+)\\/(\\d+)/ )[ 0 ][ 1 , 2 ]+[ logs ]} \\ | map { name , idx , logs -> [ name , idx . toInteger (), logs ]} \\ | groupTuple ( size: params . sp_points ) \\ | map { name , idx , logs -> [ name , idx , logs , params . merge_flags ]} \\ | merge \\ | set { ch_new_ds } //======================================================================================= // check convergence ==================================================================== ch_new_ds \\ | join ( ch_trajs ) \\ | check \\ check . out \\ | map { name , geo , msg -> \\ [( name =~ /gen(\\d+)\\/.+/ )[ 0 ][ 1 ], geo , msg . contains ( 'Converged' )]} \\ | groupTuple ( size: params . geo_size . toInteger ()) \\ | map { gen , geo , conv -> [ gen , geo , conv . every ()]} | set { nx_geo_converge } //======================================================================================= // mix the new dataset ================================================================== ch_inp . map {[ it [ 0 ], it [ 2 ]]}. set { ch_old_ds } ch_new_ds \\ | map { name , idx , ds -> [( name =~ /gen(\\d+)\\/.+/ )[ 0 ][ 1 ], ds ]} \\ | groupTuple ( size: params . geo_size . toInteger ()) \\ | join ( ch_old_ds ) \\ | map { it +[ params . new_flag , params . old_flag ]} \\ | dsmix \\ | set { nx_ds } //======================================================================================= // combine everything for new inputs ==================================================== ch_inp . map {[ it [ 0 ], it [ 4 ]]}. set { nx_step } ch_inp . map {[ it [ 0 ], it [ 5 ]]}. set { nx_time } acc_fac = params . acc_fac . toFloat () brake_fac = params . brake_fac . toFloat () min_time = params . min_time . toFloat () max_time = params . max_time . toFloat () retrain_step = params . retrain_step . toInteger () nx_geo_converge | join ( nx_models ) | join ( nx_ds ) | join ( nx_time ) | join ( nx_step ) \\ | map { gen , geo , converge , models , ds , time , step -> \\ [( gen . toInteger ()+ 1 ). toString (), geo , ds , models , \\ converge ? step : step + retrain_step , \\ converge ? Math . min ( time * acc_fac , max_time ) : Math . max ( time * brake_fac , min_time ), \\ converge ]} \\ | set { nx_inp } //======================================================================================= ch_inp . subscribe { logger ( \"[gen${it[0]}] ${it[-1]? 'not training': 'training'} the models.\" )} check . out . subscribe { name , geo , msg -> logger ( \"[$name] ${msg.trim()}\" )} nx_inp . subscribe { logger ( '-' * 80 + '\\n' + \"[gen${it[0]}] next time scale ${it[5]} ps, ${it[6] ? 'no training planned' : 'next training step '+it[4]}.\" ) } \\ emit: nx_inp }","title":"Parameters"},{"location":"tutorial/configure/","text":"Profiles and Configuration The nextflow.config file In your workflow folder, you will find a file named nextflow.config that looks like this: includeConfig 'profiles/standard.config' includeConfig 'profiles/teoroo2.config' includeConfig 'profiles/alvis.config' The nextflow.config file contains detailed specifications about the resources to be used by each process, it can also change default parameters in the workflow. The configuration file separates the definition from that of the workflow itself, which means the same workflow will be runnable on any resources supported by nextflow (from local computer, to HPC computer, to cloud-based platforms). Available options can be found in the nextflow documentation . In PiNNAcLe, those configurations are always defined as \"profiles\", so that they can be switched with the -profile name option for nextflow run . PiNNAcLe curates a list of profiles for some of the computational resources we have access to. Those can be good starting points for you to adapt for your own need. You are also welcome to contribute your config if think it will be useful for others.","title":"Configure"},{"location":"tutorial/configure/#profiles-and-configuration","text":"","title":"Profiles and Configuration"},{"location":"tutorial/configure/#the-nextflowconfig-file","text":"In your workflow folder, you will find a file named nextflow.config that looks like this: includeConfig 'profiles/standard.config' includeConfig 'profiles/teoroo2.config' includeConfig 'profiles/alvis.config' The nextflow.config file contains detailed specifications about the resources to be used by each process, it can also change default parameters in the workflow. The configuration file separates the definition from that of the workflow itself, which means the same workflow will be runnable on any resources supported by nextflow (from local computer, to HPC computer, to cloud-based platforms). Available options can be found in the nextflow documentation . In PiNNAcLe, those configurations are always defined as \"profiles\", so that they can be switched with the -profile name option for nextflow run . PiNNAcLe curates a list of profiles for some of the computational resources we have access to. Those can be good starting points for you to adapt for your own need. You are also welcome to contribute your config if think it will be useful for others.","title":"The nextflow.config file"},{"location":"tutorial/get_started/","text":"Get Started In this demo, we prepare a dataset with the xTB for different water/ethanol mixtures, and run an activated learning workflow starting from the initial dataset. Installation PiNNAcLe depends on nextflow, it can be installed following the official documentation . By default, the workflows will be run through singularity containers, and you need to install that as well, see their documentation . Singularity is the recommended way of running the workflows, it is intended for running and sharing software, and avoids complications with compilation and environment setup. But if you prefer otherwise, you can easily configure workflow to run with a different profile, which controls how and on what resource each process is run, read more in the Profiles section. Get the workflow The PiNNAcLe workflow can be downloaded as a copier template. The template simply downloads the PiNNAcLe modules and sets up a project directory ready to be extended. If you do not have copier installed, you can simply clone the repo. copier gh:teoroo-cmc/pinnacle demo_proj # or git clone https://github.com/Teoroo-CMC/PiNNAcLe.git demo_proj You may also run the workflow directly as a shared pipeline, as demonstrated here , we chose to have the modules explicitly in the project folder so that you can see how they can be adapted to your need in later sections of the tutorial. Running the workflow To run the workflow: nextflow run main.nf # add -profile your_profile if necessary You might need to wait a while for the singularity images to be pulled, afterward you should see something like this: Launching `main.nf` [friendly_carlsson] DSL2 - revision: e28d0f3d5d executor > local (18) [0d/2b1c2a] process > we_demo:mol2box (2) [100%] 9 of 9 \u2714 [3b/ebc632] process > we_demo:md:aseMD (3) [ 0%] 0 of 6 [- ] process > we_demo:mkds - [- ] process > we_demo:mkgeo - [- ] process > we_demo:acle:loop:train - [- ] process > we_demo:acle:loop:md - [- ] process > we_demo:acle:loop:convert - [- ] process > we_demo:acle:loop:sp:aseSP - [- ] process > we_demo:acle:loop:merge - [- ] process > we_demo:acle:loop:check - [- ] process > we_demo:acle:loop:dsmix - What listed above as \"processes\" are the basic element of nextflow workflows. A workflow defines how different processes are linked, and the configuration controls how they are launched in practice (e.g., environment setup, parallelization, resource allocation, etc.) You might notice that the processes are tracked by a 2-level hex code such as 0d/2d1c2a . This is how nextflow arrange the output of processes, each process will be launched in a folder named after this unique code. The code will be useful for restarting the workflow and for debugging the processes, the folder structure is illustrated below. demo_proj \u251c\u2500\u2500 main.nf # main entry point for the workflows \u251c\u2500\u2500 nextflow # nextflow recipes and modules \u2502 \u251c\u2500\u2500 acle.nf \u2502 \u251c\u2500\u2500 we_demo.nf \u2502 \u2514\u2500\u2500 module \u2502 \u251c\u2500\u2500 pinn.nf \u2502 \u251c\u2500\u2500 tips.nf \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 we_demo # output folders are linke to work directory \u2502 \u251c\u2500\u2500 init \u2502 \u2502 \u251c\u2500\u2500 geo \u2502 \u2502 \u2502 \u251c\u2500\u2500 e32-1.xyz -> /xx/demo_proj/work/c6/322d1c1... \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 work # actual execution directory of the workflow \u251c\u2500\u2500 c6 \u2502 \u251c\u2500\u2500 322d1c1f55ec381f34c361bc4e743f \u2502 \u2502 \u251c\u2500\u2500 e32-1.xyz \u2502 \u2502 \u251c\u2500\u2500 .commnad.sh \u2502 \u2502 \u251c\u2500\u2500 .commnad.run \u2502 \u2502 \u251c\u2500\u2500 .commnad.out \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 ... Useful commands Before going further, below are some nextflow commands that you might find useful. For a more comprehensive description, check the nextflow CLI reference . Checking Nextflow keeps track of what you have run and how your run them Each run of the workflow is attached to a \"run name\", with which you can resume, log, and clean up the jobs, try nextflow log tiny_brattain (replace with your own run) to get a list of all jobs in a run. $ nextflow log --- TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2022-04-26 22:52:18 1h 40m 53s tiny_brattain OK e7132a82d7 bf527311-a5d9-4f7b-b615-d50ff99e6ec5 nextflow run main.nf Resuming Nextflow caches the jobs according to their inputs, if your run is interrupted, you can resume from where your stopped with the -resume . You can also specify a previous sessuib to resume from: nextflow run main.nf -resume SESSION_ID $ nextflow run main.nf -resume --- Launching `main.nf` [disturbed_crick] - revision: 3439ecc683 executor > local (6) [85/e10f9e] process > train (1) [50%] 3 of 6, cached: 3 Cleaning Keeping a complete record of things has a cost, the script and intermediate results for past runs might accumulate to a large number of file. It is recommended to clean up your unneeded runs regularly. If you are restricted by disk, you should also consider setting the scratch directive. $ nextflow clean -f disturbed_crick --- Removed /home/user/proj/work/3f/f350726963c2372d1673b900553d6f Removed /home/user/proj/work/83/60f47f79880256257fc53a19b6ffac Removed /home/user/proj/work/5f/bd7e2a391cb53a4e20057c4641e408 What next? Workflow : more on how to run workflows; Configure : how the processes are executed.","title":"Get started"},{"location":"tutorial/get_started/#get-started","text":"In this demo, we prepare a dataset with the xTB for different water/ethanol mixtures, and run an activated learning workflow starting from the initial dataset.","title":"Get Started"},{"location":"tutorial/get_started/#installation","text":"PiNNAcLe depends on nextflow, it can be installed following the official documentation . By default, the workflows will be run through singularity containers, and you need to install that as well, see their documentation . Singularity is the recommended way of running the workflows, it is intended for running and sharing software, and avoids complications with compilation and environment setup. But if you prefer otherwise, you can easily configure workflow to run with a different profile, which controls how and on what resource each process is run, read more in the Profiles section.","title":"Installation"},{"location":"tutorial/get_started/#get-the-workflow","text":"The PiNNAcLe workflow can be downloaded as a copier template. The template simply downloads the PiNNAcLe modules and sets up a project directory ready to be extended. If you do not have copier installed, you can simply clone the repo. copier gh:teoroo-cmc/pinnacle demo_proj # or git clone https://github.com/Teoroo-CMC/PiNNAcLe.git demo_proj You may also run the workflow directly as a shared pipeline, as demonstrated here , we chose to have the modules explicitly in the project folder so that you can see how they can be adapted to your need in later sections of the tutorial.","title":"Get the workflow"},{"location":"tutorial/get_started/#running-the-workflow","text":"To run the workflow: nextflow run main.nf # add -profile your_profile if necessary You might need to wait a while for the singularity images to be pulled, afterward you should see something like this: Launching `main.nf` [friendly_carlsson] DSL2 - revision: e28d0f3d5d executor > local (18) [0d/2b1c2a] process > we_demo:mol2box (2) [100%] 9 of 9 \u2714 [3b/ebc632] process > we_demo:md:aseMD (3) [ 0%] 0 of 6 [- ] process > we_demo:mkds - [- ] process > we_demo:mkgeo - [- ] process > we_demo:acle:loop:train - [- ] process > we_demo:acle:loop:md - [- ] process > we_demo:acle:loop:convert - [- ] process > we_demo:acle:loop:sp:aseSP - [- ] process > we_demo:acle:loop:merge - [- ] process > we_demo:acle:loop:check - [- ] process > we_demo:acle:loop:dsmix - What listed above as \"processes\" are the basic element of nextflow workflows. A workflow defines how different processes are linked, and the configuration controls how they are launched in practice (e.g., environment setup, parallelization, resource allocation, etc.) You might notice that the processes are tracked by a 2-level hex code such as 0d/2d1c2a . This is how nextflow arrange the output of processes, each process will be launched in a folder named after this unique code. The code will be useful for restarting the workflow and for debugging the processes, the folder structure is illustrated below. demo_proj \u251c\u2500\u2500 main.nf # main entry point for the workflows \u251c\u2500\u2500 nextflow # nextflow recipes and modules \u2502 \u251c\u2500\u2500 acle.nf \u2502 \u251c\u2500\u2500 we_demo.nf \u2502 \u2514\u2500\u2500 module \u2502 \u251c\u2500\u2500 pinn.nf \u2502 \u251c\u2500\u2500 tips.nf \u2502 \u2514\u2500\u2500 ... \u251c\u2500\u2500 we_demo # output folders are linke to work directory \u2502 \u251c\u2500\u2500 init \u2502 \u2502 \u251c\u2500\u2500 geo \u2502 \u2502 \u2502 \u251c\u2500\u2500 e32-1.xyz -> /xx/demo_proj/work/c6/322d1c1... \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 work # actual execution directory of the workflow \u251c\u2500\u2500 c6 \u2502 \u251c\u2500\u2500 322d1c1f55ec381f34c361bc4e743f \u2502 \u2502 \u251c\u2500\u2500 e32-1.xyz \u2502 \u2502 \u251c\u2500\u2500 .commnad.sh \u2502 \u2502 \u251c\u2500\u2500 .commnad.run \u2502 \u2502 \u251c\u2500\u2500 .commnad.out \u2502 \u2502 \u2514\u2500\u2500 ... \u2502 \u2514\u2500\u2500 ... \u2514\u2500\u2500 ...","title":"Running the workflow"},{"location":"tutorial/get_started/#useful-commands","text":"Before going further, below are some nextflow commands that you might find useful. For a more comprehensive description, check the nextflow CLI reference . Checking Nextflow keeps track of what you have run and how your run them Each run of the workflow is attached to a \"run name\", with which you can resume, log, and clean up the jobs, try nextflow log tiny_brattain (replace with your own run) to get a list of all jobs in a run. $ nextflow log --- TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2022-04-26 22:52:18 1h 40m 53s tiny_brattain OK e7132a82d7 bf527311-a5d9-4f7b-b615-d50ff99e6ec5 nextflow run main.nf Resuming Nextflow caches the jobs according to their inputs, if your run is interrupted, you can resume from where your stopped with the -resume . You can also specify a previous sessuib to resume from: nextflow run main.nf -resume SESSION_ID $ nextflow run main.nf -resume --- Launching `main.nf` [disturbed_crick] - revision: 3439ecc683 executor > local (6) [85/e10f9e] process > train (1) [50%] 3 of 6, cached: 3 Cleaning Keeping a complete record of things has a cost, the script and intermediate results for past runs might accumulate to a large number of file. It is recommended to clean up your unneeded runs regularly. If you are restricted by disk, you should also consider setting the scratch directive. $ nextflow clean -f disturbed_crick --- Removed /home/user/proj/work/3f/f350726963c2372d1673b900553d6f Removed /home/user/proj/work/83/60f47f79880256257fc53a19b6ffac Removed /home/user/proj/work/5f/bd7e2a391cb53a4e20057c4641e408","title":"Useful commands"},{"location":"tutorial/get_started/#what-next","text":"Workflow : more on how to run workflows; Configure : how the processes are executed.","title":"What next?"},{"location":"tutorial/workflow/","text":"Understanding the workflow Parameters So far in the tutorial, we have executed the workflow as-is. In nextflow, there is a built-in way of defining variables that can be tweaked at runtime, called \"parameters\". See the nextflow/we_demo.nf as an example: params . proj = 'we_demo' params . init_flags = '--t 0.10 --log-every 1' // 200 steps each params . init_seeds = 2 params . init_steps = 200000 params . init_model = 'input/pinn/pinet-hco-adam.yml' params . init_time = 0.5 The params.proj parameter determines where the output files will be \"published\" into. To change while running the same workflow, we simply have to add the option to nextflow run , i.e. the following command will run the same workflow, but use 2 initial seeds (to generate the initial dataset with). nextflow run main.nf --proj = trail2 --init_seeds = 2 Modules and inclusion Thanks to the DSL2 syntax of nextflow, the workflows in PiNNAcLe are reusable easily. Below, on see how the workflow parameters can be injected into included processes/workflows; the publishing directories of subworkflows is set, and a few parameters required by acle are supplied: include { mol2box } from \"./module/molutils.nf\" addParams ( publish: \"$params.proj/init/pack\" ) include { md } from \"./module/dftb.nf\" addParams ( publish: \"$params.proj/init/md/\" ) include { convert as mkgeo } from \"./module/tips.nf\" addParams ( publish: \"$params.proj/init/ds\" ) include { convert as mkds } from \"./module/tips.nf\" addParams ( publish: \"$params.proj/init/geo\" ) include { acle } from \"./acle.nf\" addParams ( publish: \"$params.proj/acle\" , geo_size: geo_size ) Processes The processes, such as the ones included above are the building block of nextflow scripts. As shown below, they are simply scripts with a few input/output \"channels\". When the workflow runs, the scripts will be executed in the work directory, where the $variables will be substituted by whatever supplied by the input channels; and the files produced by the output patterns will. process aseSP { label \"$params.ase_label\" publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( calc , stageAs: 'calc.py' ), path ( geo ), path ( aux ) output: tuple val ( name ), path ( \"sp.xyz\" ) script: \"\"\" #!/usr/bin/env python from ase.io import read, write from calc import calc atoms = read('$geo') atoms.calc=calc calc.calculate(atoms, properties=${params.ase_props}) write('sp.xyz', atoms) \"\"\" } It should be easy to tweak the workflow behavior via the process scripts. You might also want to check the nextflow documentation to see what is possible. In addition, a few implementation patterns are followed: processes are labelled by the dependent software and arranged as \"modules\"; interchangeable processes in different modules share the same name (e.g. sp , md ); 1 processes input is always one tuple, where the first element is indented as an identifier; 2 Workflow and recipes IN DSL2, workflows are composed of processes, and can share the same input/output structure to behave like a process. However, the \"main\" workflows to be executed would have to be self-contained, meaning it will not receive inputs and will define the \"channels\" within itself and feed them to the sub-workflows, as shown in the following snippet. workflow entry { ch_inputs = Channel . fromList ( inputs ) \\ | combine ( Channel . of ( 1 .. params . init_seeds )) \\ | map { name , tag , box , seed -> [ \"$name-$seed\" , tag , box . toFloat (), seed ]} \\ | mol2box Those modules that provide self-contained workflow are called \"recipes\" in PiNNAcLe. The workflow will always be named as entry and exposed as the module name in the main workflow, i.e., the following three commands does the same thing: nextflow run nextflow/acle.nf nextflow run nextflow/acle.nf -entry entry nextflow run main.nf -entry acle I.e., the -entry option chooses which workflow to run within a workflow file, and the main.nf serves as a shortcut to call those recipes. Other than that, there are no qualitative difference between recipes and normal modules, and both can expose reusable sub-workflows, such as the acle workflow in the demo. This also applies to workflows that shares a common input/output pattern. This allows complex workflows to be composed in a implementation-agnostic fashion, for instance, the reference ( params.ref ) and machine learning potential ( params.mlp ) can be swapped to any compatible module. \u21a9 This ensures all outputs are trackable by their identifier in a complex workflow. For instance, in the acle workflow, the labelled points are identified as gen/geo/idx , the information is used to map them back to the sampled trajectory when evaluating the performance of the model. \u21a9","title":"Workflow"},{"location":"tutorial/workflow/#understanding-the-workflow","text":"","title":"Understanding the workflow"},{"location":"tutorial/workflow/#parameters","text":"So far in the tutorial, we have executed the workflow as-is. In nextflow, there is a built-in way of defining variables that can be tweaked at runtime, called \"parameters\". See the nextflow/we_demo.nf as an example: params . proj = 'we_demo' params . init_flags = '--t 0.10 --log-every 1' // 200 steps each params . init_seeds = 2 params . init_steps = 200000 params . init_model = 'input/pinn/pinet-hco-adam.yml' params . init_time = 0.5 The params.proj parameter determines where the output files will be \"published\" into. To change while running the same workflow, we simply have to add the option to nextflow run , i.e. the following command will run the same workflow, but use 2 initial seeds (to generate the initial dataset with). nextflow run main.nf --proj = trail2 --init_seeds = 2","title":"Parameters"},{"location":"tutorial/workflow/#modules-and-inclusion","text":"Thanks to the DSL2 syntax of nextflow, the workflows in PiNNAcLe are reusable easily. Below, on see how the workflow parameters can be injected into included processes/workflows; the publishing directories of subworkflows is set, and a few parameters required by acle are supplied: include { mol2box } from \"./module/molutils.nf\" addParams ( publish: \"$params.proj/init/pack\" ) include { md } from \"./module/dftb.nf\" addParams ( publish: \"$params.proj/init/md/\" ) include { convert as mkgeo } from \"./module/tips.nf\" addParams ( publish: \"$params.proj/init/ds\" ) include { convert as mkds } from \"./module/tips.nf\" addParams ( publish: \"$params.proj/init/geo\" ) include { acle } from \"./acle.nf\" addParams ( publish: \"$params.proj/acle\" , geo_size: geo_size )","title":"Modules and inclusion"},{"location":"tutorial/workflow/#processes","text":"The processes, such as the ones included above are the building block of nextflow scripts. As shown below, they are simply scripts with a few input/output \"channels\". When the workflow runs, the scripts will be executed in the work directory, where the $variables will be substituted by whatever supplied by the input channels; and the files produced by the output patterns will. process aseSP { label \"$params.ase_label\" publishDir \"$params.publish/$name\" input: tuple val ( name ), path ( calc , stageAs: 'calc.py' ), path ( geo ), path ( aux ) output: tuple val ( name ), path ( \"sp.xyz\" ) script: \"\"\" #!/usr/bin/env python from ase.io import read, write from calc import calc atoms = read('$geo') atoms.calc=calc calc.calculate(atoms, properties=${params.ase_props}) write('sp.xyz', atoms) \"\"\" } It should be easy to tweak the workflow behavior via the process scripts. You might also want to check the nextflow documentation to see what is possible. In addition, a few implementation patterns are followed: processes are labelled by the dependent software and arranged as \"modules\"; interchangeable processes in different modules share the same name (e.g. sp , md ); 1 processes input is always one tuple, where the first element is indented as an identifier; 2","title":"Processes"},{"location":"tutorial/workflow/#workflow-and-recipes","text":"IN DSL2, workflows are composed of processes, and can share the same input/output structure to behave like a process. However, the \"main\" workflows to be executed would have to be self-contained, meaning it will not receive inputs and will define the \"channels\" within itself and feed them to the sub-workflows, as shown in the following snippet. workflow entry { ch_inputs = Channel . fromList ( inputs ) \\ | combine ( Channel . of ( 1 .. params . init_seeds )) \\ | map { name , tag , box , seed -> [ \"$name-$seed\" , tag , box . toFloat (), seed ]} \\ | mol2box Those modules that provide self-contained workflow are called \"recipes\" in PiNNAcLe. The workflow will always be named as entry and exposed as the module name in the main workflow, i.e., the following three commands does the same thing: nextflow run nextflow/acle.nf nextflow run nextflow/acle.nf -entry entry nextflow run main.nf -entry acle I.e., the -entry option chooses which workflow to run within a workflow file, and the main.nf serves as a shortcut to call those recipes. Other than that, there are no qualitative difference between recipes and normal modules, and both can expose reusable sub-workflows, such as the acle workflow in the demo. This also applies to workflows that shares a common input/output pattern. This allows complex workflows to be composed in a implementation-agnostic fashion, for instance, the reference ( params.ref ) and machine learning potential ( params.mlp ) can be swapped to any compatible module. \u21a9 This ensures all outputs are trackable by their identifier in a complex workflow. For instance, in the acle workflow, the labelled points are identified as gen/geo/idx , the information is used to map them back to the sampled trajectory when evaluating the performance of the model. \u21a9","title":"Workflow and recipes"}]}